{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-9-Paradigm-Of-ML/blob/main/RBF_Kernel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3WdSSeCKD87"
      },
      "source": [
        "## Perform Non-Linear/Kernelized SVM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JUUi1e6ME98"
      },
      "source": [
        "In this experiment, we will use make_circles() dataset from sklearn. This make_circles() function generates a binary classification problem with datasets that fall into concentric circles. This function is suitable for algorithms that can learn complex non-linear manifolds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSscvWHVsZQO"
      },
      "source": [
        "### Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtsONQiNscVi"
      },
      "source": [
        "The kernel means transforming data into another dimension that has a clear dividing margin between classes of data as shown in the figure below.\n",
        "\n",
        "<center>\n",
        "<img src= \"https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/kernel.png\" width= 500 px/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC8U_MiJspYS"
      },
      "source": [
        "### Kernelized SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUQmsoistsa"
      },
      "source": [
        "The objective function for linear SVM is given by\n",
        "\n",
        "$$L(\\alpha) = \\sum \\limits_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum \\limits_{i=1}^{m} \\sum \\limits_{j=1}^{m} \\alpha_i\\alpha_jt_it_jx_i^T.x_j$$\n",
        "\n",
        "The objective function for linear SVM after $\\phi$ transformation\n",
        "\n",
        "$$L(\\alpha) = \\sum \\limits_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum \\limits_{i=1}^{m} \\sum \\limits_{j=1}^{m} \\alpha_i\\alpha_jt_it_j\\phi (x_i)^T.\\phi (x_j)$$\n",
        "\n",
        "where $\\alpha$ is the Lagrange multiplier such that $\\alpha_i \\geq 0$ for $i = 1, \\cdots, m$ and $\\sum \\limits_{i=1}^{m}\\alpha_it_i = 0$.\n",
        "\n",
        "The objective function after transformation is more expensive to evaluate than the previous one and it leads to the following modified form\n",
        "\n",
        "$$L(\\alpha) = \\sum \\limits_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum \\limits_{i=1}^{m} \\sum \\limits_{j=1}^{m} \\alpha_i\\alpha_jt_it_jK(x_i,x_j)$$\n",
        "\n",
        "where $K(x_i,x_j) = \\phi (x_i)^T.\\phi (x_j)$ is called a kernel function.\n",
        "\n",
        "Hence, in machine learning, a kernel is a function capable of computing the dot product $ϕ(x_i)^T ϕ(x_j)$ based only on the original vectors $x_i$ and $x_j$, without having to compute (or even to know about) the transformation $ϕ$. This is the essence of the kernel trick.\n",
        "\n",
        "Some of the most commonly used kernels are:\n",
        "\n",
        "* Linear: \n",
        "$$K(x_i, x_j) = x_i^Tx_j$$\n",
        "* Polynomial: \n",
        "$$K(x_i, x_j) = (\\gamma x_i^Tx_j + r)^d$$\n",
        "* Gaussian Radial Basis Function (RBF): \n",
        "$$K(x_i, x_j) = exp(-\\gamma ||x_i - x_j||^2)$$\n",
        "* Sigmoid: \n",
        "$$K(x_i, x_j) = tanh(\\gamma x_i^Tx_j + r)$$\n",
        "\n",
        "where $x_i$ and $x_j$ are original vectors, $d$ is degree of polynomial, $r$ is free parameter and $\\gamma$ is regularization parameter.\n",
        "\n",
        "One of the most commonly used kernels for SVM is RBF kernel. Let's see how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH8xSH_ovPF3"
      },
      "source": [
        "### Working of RBF Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v0G-2djvUMK"
      },
      "source": [
        "One of the techniques to tackle nonlinear problems is to add features computed using a similarity function that measures how much each instance resembles a particular landmark. \n",
        "\n",
        "For example, let’s take the 1D dataset add two landmarks to it at $x_1  = –2$ and $x_1  = 1$ (see left plot in the figure below). Next, let’s define the similarity function to be the Gaussian Radial Basis Function (RBF) with $\\gamma = 0.3$.\n",
        "\n",
        "$$\\phi_{\\gamma}(x, l) = exp(-\\gamma||x-l||^2)$$\n",
        "\n",
        "where, $l$ is the landmark (which can be another datapoint).\n",
        "\n",
        "It is a bell-shaped function varying from $0$ (very far away from the landmark) to $1$ (at the landmark). \n",
        "\n",
        "Now we compute the new features, let’s look at the instance $x_1  = –1$: it is located at a distance of $1$ from the first landmark, and $2$ from the second landmark. Therefore its new features are \n",
        "* $x_2 = exp (–0.3 × 1^2 ) ≈ 0.74$ and \n",
        "* $x_3 = exp (–0.3 × 2^2) ≈ 0.30$. \n",
        "\n",
        "The plot on the right of the below figure shows the transformed dataset. As we can see, it is now linearly separable.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/RBFKernel_plots.png\" />\n",
        "<figcaption>Feature space before transformation$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $Feature space after transformation</figcation>\n",
        "</center>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install mlxtend --upgrade --no-deps"
      ],
      "metadata": {
        "id": "KIvofVYnYyU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RYdjAlmuLVZ"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_circles\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.plotting import plot_decision_regions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYk-ReEiM9Tf"
      },
      "source": [
        "### Load and Visualize the Data\n",
        "\n",
        "Load the data from the SKlearn datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBeEAxQtNANt"
      },
      "source": [
        "# The number of points generated is 100 \n",
        "# The scale factor between inner and outer circle is 0.1. Inner circle is one class and outer circle is another class.\n",
        "# The Standard deviation of Gaussian noise added to the data is 0.1\n",
        "\n",
        "X, y = make_circles(200, factor = .1, noise = .1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bzi1bdfNFiu"
      },
      "source": [
        "To get a sense of the data, let us visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBn1mQlMNGhl"
      },
      "source": [
        "# c is the color sequence which assigns colors based on the no.of labels\n",
        "# s is the marker size in the plot\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVVt9kgfNeNt"
      },
      "source": [
        "### Try to separate the data by applying SVM linear classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCfbWD7aNnik"
      },
      "source": [
        "Apply the SVM classifier and try to fit the model using Linear Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hYX-nHuNp-r"
      },
      "source": [
        "clf_linear = SVC(kernel='linear').fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kx0KWVzNsrc"
      },
      "source": [
        "\n",
        "\n",
        "Let us visualize the decision boundaries of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvpDB6MfNvpc"
      },
      "source": [
        "plot_decision_regions(X, y, clf_linear, legend=2);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTvWUmvxN5zG"
      },
      "source": [
        "From the above plot, observe that the data points are not linearly seperable by using linear SVM model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSCocfHJOSqc"
      },
      "source": [
        "### How to work with non-linear separable data in SVM?\n",
        "\n",
        "* Transform a two-dimensional dataset onto a new three-dimensional feature space (higher dimensional space) via a mapping function where the classes become separable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieQW9Ec4OdmC"
      },
      "source": [
        "#### Mapping Function (Radial basis function) \n",
        "\n",
        "* The Radial basis function is commonly used in support vector machine classification. RBF can map an input space in infinite dimensional space.\n",
        "* By using Radial basis function add one more dimension to the original data to visualize the data linearly in high dimensional space\n",
        "* Below is the formula to compute RBF function. The gamma value ranges between 0 to 1. Here take gamma = 1\n",
        "\n",
        "   $K(X, X_i) = exp(-\\gamma * \\sum(X-X_i)^2)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kq4KswBO-bc"
      },
      "source": [
        "# Radial Basis Function where gamma = 1 and taking landmark at (0,0)\n",
        "gamma = 1\n",
        "landmark = np.array([0.0, 0.0])\n",
        "rbf = np.exp(-gamma * np.sum((X - landmark)**2, axis = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEJmbGaNPmyk"
      },
      "source": [
        "# Visualze data in 3d\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(X[:, 0], X[:, 1], rbf, c=y, s=20, cmap='autumn')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk8wowhgP9FH"
      },
      "source": [
        "From the above plot, observe that the data becomes linearly separable by transforming the data to a higher dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msQ2BO14QA9L"
      },
      "source": [
        "This type of basis function transformation is known as a kernel transformation, as it is based on a similarity relationship (or kernel) between each pair of points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKg8O5r2QKAX"
      },
      "source": [
        "### Try to apply SVM Classifier using RBF Kernel\n",
        "\n",
        "In Scikit-Learn, we can apply kernelized SVM simply by specifying kernel to RBF (radial basis function) kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrFwhX5IQTn1"
      },
      "source": [
        "# Kernel is 'rbf'\n",
        "clf_RBF = SVC(kernel='rbf').fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlYX9u1oQbCi"
      },
      "source": [
        "Visualization using RBF Kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgxXvHS2QcoJ"
      },
      "source": [
        "plot_decision_regions(X, y, clf_RBF, legend=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWhO4lskcik8"
      },
      "source": [
        "Using the RBF kernelized support vector machine, we see a suitable nonlinear decision boundary."
      ]
    }
  ]
}