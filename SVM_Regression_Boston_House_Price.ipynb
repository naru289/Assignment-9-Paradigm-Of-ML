{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-9-Paradigm-Of-ML/blob/main/SVM_Regression_Boston_House_Price.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint"
      ],
      "metadata": {
        "id": "qgFikQQa-lgU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCUz0mCt7AT8"
      },
      "source": [
        "## Support Vector Machine - Regression (SVR)\n",
        "\n",
        "Support Vector Regression is a supervised learning algorithm that is used to predict discrete values. Support Vector Regression uses the same principle as the SVMs. The basic idea behind SVR is to find the best fit line. In SVR, the best fit line is the hyperplane that has the maximum number of points.\n",
        "\n",
        "\n",
        "To use SVMs for regression instead of classification, the trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting\n",
        "margin violations, SVM Regression tries to fit as many instances as possible\n",
        "on the street while limiting margin violations (i.e., instances off the street). The width of the street is controlled by a hyperparameter, ε\n",
        "\n",
        "![Image](https://www.saedsayad.com/images/SVR_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W9qzzAJTwbp"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "The problem that we are going to solve here is that given a set of features that describe a house in Boston, our machine learning model must predict the house price. \n",
        "\n",
        "Following are the details of each feature/attribute of the given dataset.\n",
        "\n",
        "In the dataset, each row describes a boston town or suburb. There are 506 rows and 13 attributes (features) with a target column MEDV (price)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkpj5kYrUU1D"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "The Boston House Price Dataset involves the prediction of a house price in thousands of dollars given details of the house and its neighborhood.\n",
        "\n",
        "The dataset contains 506 rows and 14 columns. It consists of price of houses in various locations in Boston. Along with price, the dataset also provides information such as :\n",
        "\n",
        "* CRIM - per capita crime rate by town.\n",
        "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "* INDUS - proportion of non-retail business acres per town.\n",
        "* CHAS- Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
        "* NOX - nitrogen oxides concentration (parts per 10 million).\n",
        "* RM - average number of rooms per dwelling.\n",
        "* AGE - proportion of owner-occupied units built prior to 1940.\n",
        "* DIS - weighted mean of distances to five Boston employment centres.\n",
        "* RAD - index of accessibility to radial highways.\n",
        "* TAX - full-value property-tax rate per $10,000$.\n",
        "* PTRATIO - pupil-teacher ratio by town.\n",
        "* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
        "* LSTAT - Percentage of lower status of the population\n",
        "* MEDV (price) - Median value of owner-occupied homes in $1000s - target column\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_KL_mXcZzJuC"
      },
      "source": [
        "#@title Run the cell to download the dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Housing_data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required packages"
      ],
      "metadata": {
        "id": "F16W5VKD_d-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np        # basic library to work with arrays\n",
        "import seaborn as sns     # library for statistical data visualization\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "gSt6Gq-d_hQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK9fwApIU3Rm"
      },
      "source": [
        "#### Loading the Boston dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy-LRoWHTrL1"
      },
      "source": [
        "boston_data = pd.read_csv(\"Housing_data.csv\")\n",
        "boston_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0X0PoD1bne6"
      },
      "source": [
        "# Check the shape of dataframe\n",
        "boston_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBNHlsu6ESes"
      },
      "source": [
        "#### Check for missing values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AnNnPNJESes"
      },
      "source": [
        "# Check for missing values\n",
        "boston_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prR0JABuESet"
      },
      "source": [
        "There is no missing values in the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1_pfE1-VZjU"
      },
      "source": [
        "# Viewing the data statistics\n",
        "boston_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeKRFx9tVgY5"
      },
      "source": [
        "# Finding out the correlation between the features\n",
        "corr = boston_data.corr()\n",
        "corr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avb23p8VESet"
      },
      "source": [
        "#### Visualization of Correlation Matrix\n",
        "\n",
        "From the below matrix, we observe that the highly correlated features has the correlation value close to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2P5eh5lWK4Y"
      },
      "source": [
        "# Plotting the heatmap of correlation between features\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.heatmap(corr, cmap = 'Wistia', annot= True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JXxy7EaXZJe"
      },
      "source": [
        "# Spliting target variable and independent variables\n",
        "X = boston_data.drop(['MEDV'], axis = 1).values # Independent features\n",
        "y = boston_data['MEDV'].values # Dependent or target feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNSpmTETbp-f"
      },
      "source": [
        "# Reshape the target variable as the MinMaxScaler expects 2D array\n",
        "y = y.reshape(len(y), 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vip8e2UESeu"
      },
      "source": [
        "### Normalizing the data\n",
        "\n",
        "For the above dataset you can observe that the ranges of different continuous variables are different. This is actually problematic. Therefore, we perform normalization for continuous variable.\n",
        "  \n",
        "The goal of normalization is to change the values of numeric columns in the dataset to a common scale without distorting differences in the ranges of values. We normalize the data to bring all the variables to the same range.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY14fJu4bip8"
      },
      "source": [
        "sc = MinMaxScaler()\n",
        "X = sc.fit_transform(X)\n",
        "y = sc.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj58lRKvESev"
      },
      "source": [
        "### Split the dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu5B653kX0Nd"
      },
      "source": [
        "# Splitting to training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2H74o_HYZ9X"
      },
      "source": [
        "### Fit the SVM Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7YfeSuDlSy_"
      },
      "source": [
        "#### Train the model using 'linear' kernel\n",
        "\n",
        "**Note:** Refer to the following [link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) for sklearn support vector regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7R2GgXelMsV"
      },
      "source": [
        "# Import SVM Regressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Create a SVM Regressor using 'linear' kernel\n",
        "linear_regressor = SVR(kernel='linear')\n",
        "\n",
        "# Train the linear model using the training sets \n",
        "linear_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Model prediction on train data using 'linear' kernel\n",
        "y_pred_linear = linear_regressor.predict(X_train)\n",
        "y_test_pred_linear = linear_regressor.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8L-HvQ1oLaM"
      },
      "source": [
        "#### Visualization of Actual vs Predicted prices (Linear Kernel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA5-1KzLlu_T"
      },
      "source": [
        "# Visualizing the differences between actual prices and predicted values\n",
        "plt.scatter(y_train, y_pred_linear)\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted prices\")\n",
        "plt.title(\"Actual Prices vs Predicted prices\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNWdl1oaZERL"
      },
      "source": [
        "#### Model Evaluation \n",
        "\n",
        "\n",
        "\n",
        "**RMSE:** Root mean squared error (RMSE) is the square root of the mean of the square of all of the error. RMSE is a way to measure the accuracy, but only to compare prediction errors of different models or model configurations for a particular variable and not between variables, as it is scale-dependent. We compute Root Mean Square Error using formula.\n",
        "\n",
        "$RMSE = \\frac{1}{n} \\sum_{i=1}^{n} ({S_i} − O_{i})^2$\n",
        "\n",
        "\n",
        "where $O_i$ are the observations, $S_i$ predicted values of a variable, and 'n'\n",
        "the number of observations available for analysis.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LAOfS9bl6fK"
      },
      "source": [
        "# Predicting Test data with the linear model\n",
        "y_test_pred_linear = linear_regressor.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN1aoitZmBiK"
      },
      "source": [
        "# Model Evaluation\n",
        "R_square = r2_score(y_test, y_test_pred_linear)\n",
        "print('R square on SVR Linear Kernel:', R_square)\n",
        "\n",
        "print('Root Mean Square Error on SVR Linear Kernel:',np.sqrt(mean_squared_error(y_test, y_test_pred_linear)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHfk4apNprIG"
      },
      "source": [
        "The R square value denotes the accuracy of the model. As the accuracy of the model increases, the R square value reaches close to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rydY-NiiYmZ2"
      },
      "source": [
        "#### Train the model using 'rbf' kernel\n",
        "\n",
        "**Note:** For more details on RBF kernel refer to the following [link](https://colab.research.google.com/drive/1VsV_5epYUBEtl9RQ36-ZbUr-XQZFKhNL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAuaZODJYgJA"
      },
      "source": [
        "# Create a SVM Regressor using 'rbf' kernel\n",
        "rbf_regressor = SVR(kernel='rbf')\n",
        "\n",
        "# Train the model using the training sets \n",
        "rbf_regressor.fit(X_train, y_train);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmdrmVrnYxnT"
      },
      "source": [
        "The follwing are the details of the parameters used in the SVM Regressor:\n",
        "\n",
        "C : float, optional (default=1.0): The penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n",
        "\n",
        "kernel : string, optional (default='rbf’): kernel parameters selects the type of hyperplane used to separate the data. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed’ or a callable.\n",
        "\n",
        "degree : int, optional (default=3): Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
        "\n",
        "gamma : float, optional (default='auto’): It is for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set. Current default is 'auto' which uses 1 / n_features.\n",
        "\n",
        "coef0 : float, optional (default=0.0): Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.\n",
        "\n",
        "shrinking : boolean, optional (default=True): Whether to use the shrinking heuristic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvLGrklaZCGS"
      },
      "source": [
        "# Model prediction on train data\n",
        "y_pred_rbf = rbf_regressor.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23ZG-WekZitI"
      },
      "source": [
        "#### Visualization of Actual vs Predicted prices (RBF Kernel)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFIOwK2GZhZR"
      },
      "source": [
        "# Visualizing the differences between actual prices and predicted values\n",
        "plt.scatter(y_train, y_pred_rbf)\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted prices\")\n",
        "plt.title(\"Actual Prices vs Predicted prices\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y766x_PZvGA"
      },
      "source": [
        "# Predicting Test data with the model\n",
        "y_test_pred_rbf = rbf_regressor.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7utvFt1Z0ed"
      },
      "source": [
        "#### Model Evaluation for the test data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enhpkgElZ4Ng"
      },
      "source": [
        "# Model Evaluation\n",
        "R_square = r2_score(y_test, y_test_pred_rbf)\n",
        "print('R square on SVR RBF Kernel:', R_square)\n",
        "print('Root Mean Square Error on SVR RBF Kernel:',np.sqrt(mean_squared_error(y_test, y_test_pred_rbf)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}