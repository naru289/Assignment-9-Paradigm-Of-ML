{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-9-Paradigm-Of-ML/blob/main/M2_AST_09_Linear_Regression_%26_Image_Classification_(CIFAR_10)_SVM_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACyro4_xpFwr"
      },
      "source": [
        "\n",
        "\n",
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment : Linear Regression & Image Classification (CIFAR-10) using Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWEKPQW2pWCM"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op3MF_vLpXBE"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand how to approach a Machine Learning problem\n",
        "* understand how to decide which algorithm to use\n",
        "* understand why we use Linear Regression \n",
        "* implement Linear Regresion using Normal Equation as well as scikit learn\n",
        "* understand CIFAR-10 dataset\n",
        "* understand the SVM Linear Classifier\n",
        "* perform Binary-Classification using SVM Linear Classifier on CIFAR-10 dataset\n",
        "* extract features using HOG (Histogram of Oriented Gradients) Method\n",
        "* perform Binary-Classification using SVM Linear Classifier on HOG features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OiFi8nj77AW"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ss-YaUTeSoFx"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M2_AST_09_Linear_Regression_&_Image_Classification_(CIFAR_10)_SVM_A\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/insurance.csv\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/DS_CIFAR-10_STD.zip\")\n",
        "    ipython.magic(\"sx unzip DS_CIFAR-10_STD.zip\")\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUi2DsSsqwFO"
      },
      "source": [
        "### Importing required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np        # basic library to work with arrays\n",
        "import seaborn as sns     # library for statistical data visualization\n",
        "import matplotlib.pyplot as plt           # basic library for plotting graphs and visualization\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "               \n",
        "import pickle\n",
        "from sklearn.svm import SVC           # importing Support vector classifier\n",
        "\n",
        "# importing confusion matrix, accuracy score, classification_report\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "HJip_I-rZ7T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLuuPgm3MrHE"
      },
      "source": [
        "## Problem Statement: Medical Insurance Expense Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO-UuAKlHwLO"
      },
      "source": [
        "### Introduction\n",
        "What is the importance of medical insurance cost prediction?\n",
        "\n",
        "If we built such a system that can help to give an estimate of insurance cost based on some conditions of the patient such as age, gender, bmi, etc, it would help insurance companies as well as individuals to know how much they have to spend on their health insurance.\n",
        "\n",
        "Here, the dataset provided to you contains attributes of a patient such as age, sex, bmi, number of children, region, smoking habit along with the corresponding medical insurance charges incured to them. \n",
        "\n",
        "You are expected to use this dataset and build a prediction model to help estimate the cost incured to the future patient given all the relevant details.\n",
        "\n",
        "Following are the details of each feature/attribute of the given dataset.\n",
        "\n",
        "* **age:** age of primary beneficiary\n",
        "\n",
        "* **sex:** insurance contractor gender, female, male\n",
        "\n",
        "* **bmi:** Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\n",
        "objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n",
        "\n",
        "* **children:** Number of children covered by health insurance / Number of dependents\n",
        "\n",
        "* **smoker:** Smoking\n",
        "\n",
        "* **region:** the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n",
        "\n",
        "* **charges:** Individual medical costs billed by health insurance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwZojrqsttQl"
      },
      "source": [
        "### Loading the Insurance dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iDKH5qF0WQd"
      },
      "source": [
        "insurance_data = pd.read_csv('insurance.csv')\n",
        "print('\\nNumber of insurance beneficiaries: {} ; features per beneficiary: {}  '.format(insurance_data.shape[0],insurance_data.shape[1]))\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_data.head()"
      ],
      "metadata": {
        "id": "mji2I4Ru5pj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaSSvTnRIKpm"
      },
      "source": [
        "### Summary of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaJ8QBwGINd7"
      },
      "source": [
        "insurance_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwUywNRFIouk"
      },
      "source": [
        "\n",
        "1. Each attribute has a full of `1338 non-null entries`, hence the dataset has no missing values. In case, if any null values(missing values) exist, we generally replace them with mean, median, or mode.\n",
        "\n",
        "2. The dataset has `4 real` and `3 categorical` attributes/features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vZeG3qxKi4P"
      },
      "source": [
        "real_cols = insurance_data.select_dtypes(exclude=['object'])\n",
        "cat_cols = insurance_data.select_dtypes(include=['object'])\n",
        "print(\"The numeric columns are: \\n{} \\n\\nThe non-numeric columns are: \\n{}\"\n",
        "      .format(list(real_cols.columns),list(cat_cols.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkXXaTKhxdZQ"
      },
      "source": [
        "When we look at the shape of dataset it returned $(1338,7)$. So there are  $m=1338$  training data and  $n=7$  features. Here, the target variable is charges and remaining six variables are age, sex, bmi, children, smoker, region are independent variable as shown earlier. There are multiple independent variable, so we need to fit Multiple linear regression. For this, the hypothesis function looks like\n",
        "\n",
        "$h_{θ}(x_{i})=θ_{0}+θ_{1}age+θ_{2}sex+θ_{3}bmi+θ_{4}children+θ_{5}smoker+θ_{6}region$\n",
        " \n",
        "This multiple linear regression equation for given dataset.\n",
        "\n",
        "If $i=1$ then\n",
        "\n",
        "$h_{θ}(x_{1})=θ_{0}+θ_{1}19+θ_{2}female+θ_{3}27.900+θ_{4}0+θ_{5}yes+θ_{6}southwest$\n",
        "\n",
        "$y_{1}=16884.92400$\n",
        "\n",
        "If $i=3$ then\n",
        "\n",
        "$h_{θ}(x_{3})=θ_{0}+θ_{1}28+θ_{2}male+θ_{3}33.000+θ_{4}3+θ_{5}no+θ_{6}northwest$\n",
        "\n",
        "$y_{3}=4449.46200$\n",
        "\n",
        "$x_{1}=(x_{11}\\hspace{0.1cm}x_{12}\\hspace{0.1cm}x_{13}\\hspace{0.1cm}x_{14}\\hspace{0.1cm}x_{15}\\hspace{0.1cm}x_{16})=(19 \\hspace{0.5cm} female \\hspace{0.5cm} 27.9001 \\hspace{0.5cm} no \\hspace{0.5cm} northwest)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoJLIqOqyUnC"
      },
      "source": [
        "### Matrix Formulation\n",
        "\n",
        "In general we can write above vector as\n",
        "\n",
        "$x_{ij}=(x_{i1}x_{i2}...x_{in})$\n",
        " \n",
        "Now, we combine all available individual vector into single input matrix of size  $(m,n)$  and denoted by  $X$  input matrix, which consist of all training examples,\n",
        "\n",
        "$X = \\begin{bmatrix}\n",
        "    x_{11}       & x_{12} & x_{13} & \\dots & x_{1n} \\\\\n",
        "    x_{21}       & x_{22} & x_{23} & \\dots & x_{2n} \\\\\n",
        "    \\vdots        & \\vdots   & \\vdots   & \\vdots  & \\vdots\\\\\n",
        "    x_{m1}       & x_{m2} & x_{m3} & \\dots & x_{mn}\n",
        "\\end{bmatrix}_{(m,n)}$\n",
        "\n",
        " $\\hat X = \\begin{bmatrix}\n",
        "   1 & x_{11}       & x_{12} & x_{13} & \\dots & x_{1n} \\\\\n",
        "   1 & x_{21}       & x_{22} & x_{23} & \\dots & x_{2n} \\\\\n",
        "    \\vdots        & \\vdots   & \\vdots   & \\vdots  & \\vdots\\\\\n",
        "   1 & x_{m1}       & x_{m2} & x_{m3} & \\dots & x_{mn}\n",
        "\\end{bmatrix}_{(m,n+1)}$\n",
        "\n",
        "We represent parameter of function and dependent variable in vector form as\n",
        "\n",
        "$\\theta = \\begin{bmatrix}\n",
        "    \\theta_{0}\\\\\n",
        "    \\theta_{1}\\\\\n",
        "    \\vdots    \\\\\n",
        "    \\theta_{j}  \\\\\n",
        "    \\vdots     \\\\\n",
        "     \\theta_{n}  \\\\\n",
        "\\end{bmatrix}_{(n+1,1)}$  $Y = \\begin{bmatrix}\n",
        "    y_{1}\\\\\n",
        "    y_{2}\\\\\n",
        "    \\vdots    \\\\\n",
        "    y_{i}  \\\\\n",
        "    \\vdots     \\\\\n",
        "    y_{m}  \\\\\n",
        "\\end{bmatrix}_{(m,1)}$  \n",
        "\n",
        " \n",
        "Here, we represent hypothesis function in vectorized form\n",
        "\n",
        "$h_{θ}(X)=X_{θ} =  X \\theta$\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8Xk1Ut3C-p_"
      },
      "source": [
        "### Visualization for Charges vs BMI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzDgGLSmyd9q"
      },
      "source": [
        "\"\"\" for our visualization purpose will fit line using seaborn library only for bmi as independent variable \n",
        "and charges as dependent variable\"\"\"\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i68qMVlFypZ3"
      },
      "source": [
        "In above plot we fit regression line into the variables\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAZd4O3syrp8"
      },
      "source": [
        "### Cost Function\n",
        "\n",
        "A cost function measures how much error is there in the model by checking in terms of ability to estimate the relationship between  x  and  y . We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference of observed dependent variable in the given the dataset and those predicted by the hypothesis function.\n",
        "\n",
        "$J(θ) = \\frac{1}{m}\\sum_{i=1}^{m} (\\hat{y}_{i}−y_{i})^2$\n",
        "\n",
        "$J(θ) = \\frac{1}{m}\\sum_{i=1}^{m} (h_{\\theta}({x}_{i})−y_{i})^2$\n",
        "  \n",
        "To implement the linear regression, you should take training example add an extra column that is  $x_{0}$  feature, where  $x_{0}$=1 .  \n",
        "\n",
        "$x_{0} =(x_{i_{0}}x_{i_{1}}x_{i_{2}}...x_{m_{i}})$ ,\n",
        "\n",
        "where  $x_{i_{0}}=0$  and input matrix will become as\n",
        "\n",
        "$X = \\begin{bmatrix}\n",
        "    x_{10}       & x_{11} & x_{12} & \\dots & x_{1n} \\\\\n",
        "    x_{20}       & x_{21} & x_{22} & \\dots & x_{2n} \\\\\n",
        "    \\vdots        & \\vdots   & \\vdots   & \\vdots  & \\vdots\\\\\n",
        "    x_{m0}       & x_{m1} & x_{m2} & \\dots & x_{mn}\n",
        "\\end{bmatrix}_{(m,n+1)}$ \n",
        "\n",
        "Each of the m input samples is similar to a column vector with n+1 rows,  $x_{0}$  being 1 for our convenience, that is  $x_{10},x_{20},x_{30}...x_{m0}=1$ . Now, we rewrite the ordinary least square cost function in matrix form as:\n",
        "\n",
        "$J(θ)=\\frac{1}{m}(X_{θ}−Y)^T(X_{θ}−Y)$\n",
        " \n",
        "Let's look at the matrix multiplication concept. The multiplication of two matrix happens only if number of column of first matrix is equal to number of rows of second matrix. Here, input matrix  X  of size  $(m,n+1)$ , parameter of function is of size  $(n+1,1)$  and dependent variable vector of size  $(m,1)$ . The product of matrix  $X(m,n+1)θ(n+1,1)$  will return a vector of size  (m,1) , then product of $(X_{θ}−Y)^T_{(1,m)}(X_{θ}−Y)_{(m,1)}$  will return size of unit vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma2GKT-kFSAX"
      },
      "source": [
        "### Normal Equation\n",
        "\n",
        "The normal equation is an analytical solution to the linear regression problem with a ordinary least square cost function. To minimize our cost function, take partial derivative of  $J(θ)$  with respect to  $θ$  and equate to  $0$. The derivative of function is nothing but checking that for a small change in input what would be the change in output of function.\n",
        "\n",
        "$min_{θ_{0},θ_{1}...θ_{n}}J(θ_{0},θ_{1}..θ_{n})$\n",
        " \n",
        "$\\frac{∂J(θ_{j})}{∂ θ_{j}}=0$\n",
        " \n",
        "where  j=0,1,2,....n \n",
        "\n",
        "Now we will apply partial derivative to our cost function,\n",
        "\n",
        "$\\frac{∂J(θ_{j})}{∂ θ_{j}}=\\frac{\\partial}{\\partial \\theta} (X_{\\theta}-Y)^T(X_{\\theta}-Y)$\n",
        "\n",
        "\n",
        " \n",
        "We will remove 1/m  since we are going to equate the derivative to 0 and solve $J(θ)$. \n",
        "\n",
        "$J(θ)=(X_{θ}−Y)^T(X_{θ}−Y)$\n",
        " \n",
        "=$(X_{θ}^T−Y^T)(X_{θ}−Y)$\n",
        " \n",
        "\n",
        "=$(Xθ)^T−Y^T)(Xθ−Y)$\n",
        " \n",
        "=$(Xθ)^T Xθ−Y^TXθ−(Xθ)^TY+Y^TY$\n",
        " \n",
        "=$θ^T X^TXθ−2(Xθ)^T Y+Y^TY$\n",
        "\n",
        "Here $\\theta$ is unknown. To find where the above function has a minimum, we will derive by $\\theta$ and equate it to 0. Also, we only use matrix notation to conveniently represent a system of linear formula. So, we derive by each component of the vector, and then combine the resulting derivatives into a vector again. \n",
        " \n",
        "$\\frac{∂J(θ)}{∂θ}=\\frac{∂}{∂θ}(θ^T X^TXθ−2θ^T X^TY+Y^TY)$\n",
        " \n",
        "$0= 2 X^TXθ−2  X^Ty$\n",
        " \n",
        "$ X^TXθ = X^TY$\n",
        " \n",
        "$θ=(X^TX)^{(−1)} X^TY$\n",
        " \n",
        "This is the normal equation for linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COv1N5HSzCAs"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLXE0fwKKw_i"
      },
      "source": [
        "Let us look into the description of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUKresQMKsEv"
      },
      "source": [
        "insurance_data.describe(include=\"all\").T "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7Lo0wOnUcYv"
      },
      "source": [
        "1. Compare means and standard devaiations for all the real columns to establish that the distributions are different. \n",
        "\n",
        "2. What is the problem if different continuous variables have different scales and magnitude?\n",
        "  \n",
        "  If there are different ranges for different continuous variables, then, the dataset will have some exceptional values very high or very low among the common values present in the dataset.   \n",
        "\n",
        "3. What is the possible solution for handling different ranges and why?\n",
        "\n",
        "  Normalizing the data is the solution. By Normalizing the data, the values can be stuffed in a specific range. Moreover, you will understand about Normalization in the later section. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9iNiVHBzUDD"
      },
      "source": [
        "#### Check for missing values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Ups59-zWNA"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWmxNzpS0Prr"
      },
      "source": [
        "There is no missing values in the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m89QcxvL0Qgn"
      },
      "source": [
        "#### Visualization of Correlation Matrix\n",
        "\n",
        "From the below matrix, we observe that the highly correlated features has the correlation value close to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf1hFJII0ST8"
      },
      "source": [
        "# Correlation plot\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9lRyI9OyIsv"
      },
      "source": [
        "#### Visualization of Distribution of Insurance Charges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5YMNISK0bXN"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blZCjUkT0oe2"
      },
      "source": [
        "If we look at the left plot the charges varies from 1120 to 63500, the plot is right skewed. In right plot we will apply natural log, then plot approximately tends to normal. For further analysis we will apply log on target variable charges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaDXky2UDhbg"
      },
      "source": [
        "#### Visualization for Charges vs Sex and Charges vs Smoker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Mm316k0pOU"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uv2AF_t0yzL"
      },
      "source": [
        "From left plot the insurance charge for male and female is approximatley in same range, it is average around 5000 bucks. In right plot the insurance charge for smokers is much wide range compare to non smokers, the average charges for non smoker is approximately 5000 bucks. For smoker the minimum insurance charge is itself 5000 bucks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-ehI6fFDvOI"
      },
      "source": [
        "#### Visualization for Charges vs Children"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrdchWR01Gs"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5rJb_8y06ys"
      },
      "source": [
        "insurance_data.groupby('children').agg(['mean','min','max'])['charges']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R15zOc-xIcB"
      },
      "source": [
        "#### Visualization for Charges vs Region"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2IpncI01ABU"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVNalvCUH_4y"
      },
      "source": [
        "#### Visualization for Charges vs Age and Charges vs BMI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBPl6e1E1E8P"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snSf0svK1Mnq"
      },
      "source": [
        "From left plot the minimum age of a person who is insured is 18 year. \n",
        "\n",
        "Body mass index (BMI) is a measure of body fat based on height and weight of a person. Here, the minimum bmi is 16 $kg/m^2$  and maximum upto 54 $kg/m^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga7VMaL-1Szz"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "#### Encoding the categorical data\n",
        "\n",
        "Machine learning algorithms cannot work with categorical data directly. So,categorical data must be converted to number. We do the conversion in three ways:\n",
        "\n",
        "1. Label Encoding\n",
        "\n",
        "2. One hot encoding\n",
        "\n",
        "3. Dummy variable trap\n",
        "\n",
        "`Label encoding` refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them.\n",
        "\n",
        "A `One hot encoding` is a representation of categorical variable as binary vectors. It allows the representation of categorical data to be more expressive. This first requires that the categorical values should be mapped to integer values, that is label encoding is performed. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with 1.\n",
        "\n",
        "The Dummy variable trap is a scenario in which the independent variable are multicollinear, a scenario in which two or more variables are highly correlated. In simple terms, one variable can be predicted from the others.\n",
        "\n",
        "Multicollinearity is a statistical calculation in which the independent or explanatory variables are interrelated to each other. Due to this multicollinearity, the model algorithm can not calculate the true relationship between dependent and explanatory variables as the outcome of the prediction has errors\n",
        "\n",
        "By using pandas `get_dummies` function we can do all above three step in line of code. We will use this fuction to get dummy variable for sex, children,smoker, and region. By setting drop_first =True, function will remove dummy variable trap by droping original variable. To know more about pandas get_dummies, click [here](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5e-zmm5106s"
      },
      "source": [
        "# Dummy variable\n",
        "categorical_columns = ['sex','children', 'smoker', 'region']\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDux0Jdtxshq"
      },
      "source": [
        "### Normalizing the data\n",
        "Earlier we have observed that the ranges of different continuous variables are different. This is actually problematic. Therefore, we perform normalization for continuous variable.\n",
        "  \n",
        "The goal of normalization is to change the values of numeric columns in the dataset to a common scale without distorting differences in the ranges of values. We normalize the data to bring all the variables to the same range.\n",
        "For this we can use `MinMaxScaler`. It scales and translates each feature individually such that it is in the given range on the training set, e.g. between `[0,1]` or else in the range `[-1, 1]` if there are negative values in the dataset. To learn more about MinMaxScaler click [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#:~:text=Transform%20features%20by%20scaling%20each,e.g.%20between%20zero%20and%20one.).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h9AQhRwHw_g"
      },
      "source": [
        "# Normalization\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Bco2t52Yet"
      },
      "source": [
        "# Checking the inverse transform to cross verify the values\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IhEQdrt6gOq"
      },
      "source": [
        "### Splitting the dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E79d5JOx6hl4"
      },
      "source": [
        "# YOUR CODE HERE : Independent feature\n",
        "# YOUR CODE HERE : Dependent features\n",
        "\n",
        "# Train test split\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ_zBBvd6uJy"
      },
      "source": [
        "### Model building\n",
        "\n",
        "#### Implementing Linear Regression using Normal Equation\n",
        "\n",
        "In this step, we will build model using our linear regression equation  $ θ=(X^TX)^{−1}X^Ty$ . In first step we need to add a feature $x_{0}=1$  to our original data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBXAck7K60rN"
      },
      "source": [
        "# Step 1: add x0 = 1 to dataset\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Step2: Building the model\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ravWNfsQ63UE"
      },
      "source": [
        "# The parameters for linear regression model\n",
        "parameter = # YOUR CODE HERE\n",
        "columns = ['intersect:x_0=1'] + list(X.columns.values)\n",
        "parameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YomYzpLtLng0"
      },
      "source": [
        "#### Implementing Linear Regression using Scikit learn\n",
        "\n",
        "1. Create an object of Linear Regression function\n",
        "  \n",
        "  *lr = LinearRegression()*\n",
        "\n",
        "2. Fit the training data(features, dependent variable) using the `fit` method from sklearn.linear_model\n",
        "\n",
        "  *fit(X_train, y_train)*\n",
        "\n",
        "3. `Predict` the results using the test data that serves as unknown features to the linear regression model.\n",
        "\n",
        "  *y_pred = lr.predict(X_test)*\n",
        "\n",
        "4. Find the `Root Mean Square Error` i.e. the difference between the predicted value and the test set value.\n",
        "\n",
        "  *rmse(y_test,y_pred)*\n",
        "\n",
        "5. Finally, find the `R2 Score` for y_test and y_pred that depicts the accuracy score of the model built.\n",
        "\n",
        "  *r2_score(y_test,y_pred)*\n",
        "\n",
        "If you want to learn more about sklearn Linear Regression, click \n",
        "[here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzQ25jpz67jV"
      },
      "source": [
        "# Scikit Learn module\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train,y_train) # Note: x_0 =1 is not required here as sklearn will take care of it.\n",
        "\n",
        "# Parameter\n",
        "sk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\n",
        "parameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\n",
        "parameter_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R49rds-x7Ceg"
      },
      "source": [
        "The parameter obtained from both the model are same. So, we succefully built our model using normal equation and verified using sklearn linear regression module. Let's move ahead with the next step of prediction and model evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHEu4HJF7ErO"
      },
      "source": [
        "### Model evaluation using Normal Equation\n",
        "\n",
        "\n",
        "We will predict value for target variable by using our model parameter for test dataset. Then, compare the predicted value with actual value in test set. We compute Mean Square Error using formula\n",
        "$J(θ)=\\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_{i}−y_{i})^2$\n",
        " \n",
        "$R^2$  is statistical measure of how close data are to the fitted regression line.  $R^2$  is always between 0 to 100%. 0% indicated that model explains none of the variability of the response data around it's mean. 100% indicated that model explains all the variablity of the response data around the mean.\n",
        "\n",
        "$R^2$ = 1−$\\frac{SSE}{SST}$\n",
        " \n",
        "SSE = Sum of Square Error\n",
        "\n",
        "SST = Sum of Square Total\n",
        "\n",
        "$SSE = \\sum_{i=1}^{m} (\\hat{y}_{i}−y_{i})^2$\n",
        "\n",
        "$SST = \\sum_{i=1}^{m} (y_{i}−\\bar{y}_{i})^2$\n",
        "\n",
        " \n",
        "Here  $\\hat{y}$  is predicted value and  $\\bar{y}$  is mean value of  y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnT_PgS-7NMn"
      },
      "source": [
        "# Normal equation\n",
        "y_pred_norm =  # YOUR CODE HERE\n",
        "\n",
        "# Evaluation: MSE\n",
        "J_mse = # YOUR CODE HERE\n",
        "\n",
        "# R_square \n",
        "sse = np.sum((y_pred_norm - y_test)**2)\n",
        "sst = np.sum((y_test - y_test.mean())**2)\n",
        "\n",
        "# YOUR CODE HERE : Calculate R square"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKjJEinz02aM"
      },
      "source": [
        "### Model Evaluation using sklearn module\n",
        "\n",
        "If you want to know more about $R^2$ score, click \n",
        "[here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)\n",
        "\n",
        "If you want to know more about Mean Square Error, click \n",
        "[here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Bf4DJV7RZP"
      },
      "source": [
        "# Prediction using sklearn regression module\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Evaluation: MSE\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# R_square\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHEMCUi1MSk2"
      },
      "source": [
        "Hence, we have successfully built and evaluated our Linear Regression model with the equation as well as the sklearn library. In both cases, we get the Mean Square Error as 0.00895630468233185 and the R square as 0.7305284299807451.\n",
        "\n",
        "The low value for MSE denotes that our model has least error values. The R square value denotes the accuracy of the model. As the accuracy of the model increases, the R square value reaches close to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Statement: Image Classification (CIFAR-10) using Support Vector Machines"
      ],
      "metadata": {
        "id": "u4HWRDBzJ5SB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NiLPMhPgYFa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9CxCComgihL"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0k4a9Qbe6p1"
      },
      "source": [
        "In this experiment, we will use the CIFAR-10 dataset. It consists of 60,000 colour images(32x32) in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images.\n",
        "\n",
        "\n",
        "The dataset is divided into five training batches and one test batch where each batch has 10000 images. The test batch contains 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:\n",
        "\n",
        "\n",
        "<img src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Images/CIFAR10.png\" alt=\"Drawing\" height=\"350\" width=\"440\"/>\n",
        "\n",
        "**The code returns the contents of each data file as a dictionary**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL8erXHHe6p2"
      },
      "source": [
        "There are 8 pickled files in the CIFAR-10 directory.\n",
        "\n",
        "    1. batches.meta\n",
        "\n",
        "    2. data_batch_1\n",
        "\n",
        "    3. data_batch_2\t\n",
        "\n",
        "    4. data_batch_3\n",
        "\n",
        "    5. data_batch_4\t\n",
        "\n",
        "    6. data_batch_5\n",
        "\n",
        "    7. readme.html\n",
        "\n",
        "    8. test_batch\n",
        "\n",
        "Getting into details of this dataset:\n",
        "\n",
        "\n",
        "**data**: A 50,000x3072 numpy array of unsigned integers. Each row of the array stores a 32x32 colour image. The first 1024 intensity values contain the red channel values, the next 1024 intensity values contain the green channel, and the final 1024 the blue channel. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image. \n",
        "\n",
        "\n",
        "**labels**: A list of 10,000 numbers from 0-9 (for the above mentioned classes airplane, automobile etc..). The number at index i indicates the label of the ith image in the array data.\n",
        "\n",
        "\n",
        "\n",
        "The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:\n",
        "\n",
        "**label_names:**  A 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPwLon_e6p4"
      },
      "source": [
        "### DataSource\n",
        "\n",
        "https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_RfJjpFG0kp"
      },
      "source": [
        "#### Function to unpickle the data\n",
        "\n",
        "To know more details about the pickle refer to the following [link](https://colab.research.google.com/drive/18lWc_k1AiSsSwbiduRTPqENCjDCfCIQZ)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4qKDf-ee6qA"
      },
      "source": [
        "# Function to unpickle the data files which is in the dictionary format\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        # Setting the encoding to latin1 allows to import the data directly\n",
        "        dict_1 = pickle.load(fo, encoding='Latin1')\n",
        "    return dict_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6119xCxOAbF"
      },
      "source": [
        "When we pass a pickled file to the get_data function it returns features, labels, file names, list of classes of the corresponding file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1_yJnrxJEcr"
      },
      "source": [
        "def get_data(file):\n",
        "    \n",
        "    # Unpickle the data file\n",
        "    dict_1 = (unpickle(file))\n",
        "\n",
        "    # Storing the features\n",
        "    X = dict_1['data']\n",
        "\n",
        "    # Storing the labels\n",
        "    Y = np.asarray(dict_1['labels'])\n",
        "\n",
        "    # Storing the .png files of images\n",
        "    file_names = dict_1['filenames']\n",
        "\n",
        "    # Get the class names \n",
        "    list_class = (unpickle(\"DS_CIFAR-10_STD/batches.meta\")['label_names'])\n",
        "\n",
        "    return X, Y, file_names, list_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706Q4tnBe6qE"
      },
      "source": [
        "### Visualizing the images in CIFAR-10 Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTnaWfKsWacC"
      },
      "source": [
        "# Read 10000 images -- from data_batch 3 by passing the file to 'get_data' function\n",
        "X, Y, names, classes = get_data(\"DS_CIFAR-10_STD/data_batch_3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFJp6weiU-AR"
      },
      "source": [
        "fig = plt.figure(figsize=(20, 15))\n",
        "plt_id = 1\n",
        "# Plotting the images by selecting the first 10 images from the 10 classes in the dataset\n",
        "for label in range(10):\n",
        "  for idx, image_id in enumerate(np.where(Y==label)[0][:10], start=1):\n",
        "    plt.subplot(10, 10, plt_id)\n",
        "    # Reshape the images with height x width x channels\n",
        "    # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7phgU6w1b29y"
      },
      "source": [
        "### Support Vector Machine - Classification (SVC)\n",
        "\n",
        "#### What is SVM?\n",
        "\n",
        "Support vector machines are supervised learning models used for classification and regression analysis. A simple linear SVM classifier works by making a straight line between two classes. That means all of the data points on one side of the line will represent a category and the data points on the other side of the line will be put into a different category. This means there can be an infinite number of lines to choose from.\n",
        "\n",
        "**Hyperplanes** are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.\n",
        " * The hyperplane with maximum margin is called the optimal hyperplane.\n",
        "\n",
        "#### What are support vectors?\n",
        "\n",
        "* Linear SVM assumes that the data is linearly separable.\n",
        "\n",
        "* It chooses the line which is more distant from both the classes.\n",
        "\n",
        "In the SVM algorithm, we find the points closest to the line from both the classes. These points are called support vectors. \n",
        "\n",
        "**Support vectors** are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Then compute the distance between the line and the support vectors which is called the margin.\n",
        "\n",
        "**Margin** is the width that the boundary could be increased by before hitting a data point.\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://cdn.talentsprint.com/aiml/aiml_2020_b14_hyd/experiment_details_backup/linear_data.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNT1Qp-DhyR_"
      },
      "source": [
        "### Let us define a function for training the Linear Support Vector Classifier\n",
        "\n",
        "For more details on SVM refer to the following [link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBvmhIla-M5O"
      },
      "source": [
        "def train(train_features, train_labels):\n",
        "    \n",
        "    # Create an instance for the LinearSVC classifier\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Fitting the data into the model\n",
        "    clf.fit(train_features, train_labels)\n",
        "\n",
        "    return clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUihkHaXiDdD"
      },
      "source": [
        "### Let us define a function to get the prediction on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47eStWTr-nBC"
      },
      "source": [
        "def predict(clf, test_features):\n",
        "    \n",
        "    # Get the prediction on the test data\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S03AOIJW6Elf"
      },
      "source": [
        "### Let us define a function to calculate accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz-ZheSN-rqe"
      },
      "source": [
        "def eval(predictions, test_labels):   \n",
        "    # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w65f_8O3ZkGZ"
      },
      "source": [
        "###  Let us unpickle the data and labels from CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFpkHmA76z46"
      },
      "source": [
        "Now let us unpickle the data and labels from CIFAR-10 dataset and divide them into training and testing sets.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsRkIdoCe6qq"
      },
      "source": [
        "train_features = []\n",
        "train_labels = []\n",
        "\n",
        "# Read all training features and labels from all the data_batch files\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Converting the train features and labels into an array\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Read all test features and labels\n",
        "test_features, test_labels, names_test, classes_test = get_data(\"DS_CIFAR-10_STD/test_batch\")\n",
        "\n",
        "# Converting the test features and labels in to an array\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTS853um7cOl"
      },
      "source": [
        "test_labels.shape, train_labels.shape, test_features.shape, train_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiOaXFw5YFKH"
      },
      "source": [
        "### Let us define a function to extract two classes to perform binary classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMSkyevFwviz"
      },
      "source": [
        "# Function to extract 2 classes to perfrom SVM Linear classification \n",
        "def pick_2classes(class0, class1, X, Y):\n",
        "\n",
        "    # Select class #0\n",
        "    X_0 = X[Y == class0]\n",
        "    Y_0 = Y[Y == class0]\n",
        "\n",
        "    # Select class #1\n",
        "    X_1 = X[Y == class1]\n",
        "    Y_1 = Y[Y == class1]\n",
        "\n",
        "    # Join the two classes (vertically row wise) to make the set\n",
        "    X_classes = np.vstack((X_0, X_1))\n",
        "    Y_classes = np.append(Y_0, Y_1)\n",
        "\n",
        "    return X_classes, Y_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGAVzdP8wx58"
      },
      "source": [
        "# Select the classes #7 and #8 for training and testing the data to get the features\n",
        "# The class 7 belongs to 'horse' and class 8 belongs to 'ship'\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao5XLuw9UJFE"
      },
      "source": [
        "# Check the shape of train and test data sets\n",
        "# YOUR CODE HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u0MdpK7CjwH"
      },
      "source": [
        "# Call the 'train' function with the train data\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Call the 'predict' function by paasing the trained classifier and the test data\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Calculate the accuracy by passing the predictions and the test labels\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luM-8iBnWY31"
      },
      "source": [
        "# Print the accuracy score\n",
        "print('Accuracy score is', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf32t_7TiSdo"
      },
      "source": [
        "**Exercise:** You can also select different class labels \n",
        "(0-9) and try passing it to the **pick_2classes** to perform the binary classification using LinearSVM and see the change in accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P_1SKeDwopi"
      },
      "source": [
        "### Preparing the data to extract the HOG features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slo3D78deYum"
      },
      "source": [
        "# Define a function to reshape the train and test features\n",
        "def batch_to_rgb(images: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given loaded images from CIFAR-10 dataset (i.e. 32x32 values\n",
        "    of red, then green and blue), returns same set of images with\n",
        "    color channel being the last, i.e. batch_to_rgb[n][y][x]\n",
        "    returns 3-valued array with r, g, b of pixel (x, y) of n-th \n",
        "    image. It returns same images with transformed colors to rgb\n",
        "    \n",
        "    \"\"\"\n",
        "    return images.reshape((-1, 3, 32, 32)).transpose(0, 2, 3, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF-iSDqbemt5"
      },
      "source": [
        "# Call the function 'batch_to_rgb' on the train and test features\n",
        "# YOUR CODE HERE\n",
        "# Check the shape of train features \n",
        "print(train_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7m41A29xtLR"
      },
      "source": [
        "\n",
        "### Feature Representation using HOG (Histogram of Oriented Gradients) Method\n",
        "\n",
        "The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The idea behind HOG is to extract features into a vector, and feed it into a classification algorithm that will assess whether a face (or any object you train it to recognize actually) is present in a region or not.\n",
        "\n",
        "Feature extraction is the process by which certain features of interest within an image are detected and represented for further processing. The resulting representation can be subsequently used as an input to the classification techniques, which will then classify, or recognize the semantic contents of the image or its objects. \n",
        "\n",
        "\n",
        "To identify the objects using HOG method requires the following steps to be followed :-\n",
        "\n",
        "\n",
        "**What is Gradient Image?**\n",
        "\n",
        "<img src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Images/Gradient.png\" alt=\"Drawing\" height=\"200\" width=\"460\"/>\n",
        "\n",
        "We start with computing the horizontal and vertical gradients of the image. In the image when we move from left to right pixel by pixel, there might be a change in the pixel value i.e, from a black lower pixel number to a white higher pixel number. Going from left to right gives us the horizontal gradient and going from top to down gives a vertical gradient. This sudden change in the color is called a gradient. \n",
        "\n",
        "The gradient image removes a lot of non-essential information (e.g. constant colored background), but highlighted outlines and still we can identify the image. At every pixel, the gradient has a magnitude and a direction. The magnitude of the gradient increases wherever there is a sharp change in intensity. \n",
        "\n",
        "**Orientation of Gradients**\n",
        "\n",
        "HOG works with a block which is similar to a sliding window. A block is considered as a pixel grid in which gradients are constituted from the magnitude and direction of change in the intensities of the pixel within the block.\n",
        "\n",
        "We first calculate the gradients by taking a block from the images. For the selected block, determine the gradient (or change) in the x-direction and aslo calculate the gradient in the y-direction. Once we get the gradients, we will also calculate the gradient magnitude and gradient angle for each pixel (in the image).\n",
        "\n",
        "**Histogram of Gradients**\n",
        "    \n",
        "The next step is to create a histogram of gradients for the block of pixels from the image. We will take each pixel value, find the angle/orientation of the pixel and update the frequency table. The same process is repeated for all the pixel values and we end up with a frequency table that denotes angles and the occurrence of these angles in the image. This frequency table can be used to generate a histogram with angle values on the x-axis and the frequency on the y-axis.\n",
        "\n",
        "From the HOG features we will find that the structure of the object is well maintained, ignoring all the insignificant features.\n",
        "\n",
        "For more details refer to the following [link](https://medium.com/analytics-vidhya/a-gentle-introduction-into-the-histogram-of-oriented-gradients-fdee9ed8f2aa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98hebNfVcLFA"
      },
      "source": [
        "# we will use the methods from scikit-image library, hog() and rgb2gray().\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "# First convert the train and test features to grayscale\n",
        "# The HOG method works for the gray scale image\n",
        "hog_train = # YOUR CODE HERE\n",
        "hog_test  = # YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Picking up the horse index from train_features\n",
        "horse = train_features[2000]\n",
        "\n",
        "# Plotting the original image\n",
        "fig, ax = plt.subplots(1, 2, figsize=(5, 5))\n",
        "ax[0].imshow(horse)\n",
        "ax[0].set_title(\"Original\")\n",
        "\n",
        "# Plotting the coverted image for the selected index of horse\n",
        "horse_gray = hog_train[2000]\n",
        "\n",
        "# Plotting the gray scale image\n",
        "ax[1].imshow(horse_gray, cmap='gray')\n",
        "ax[1].set_title(\"Grayscaled\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcsHR6kugRqH"
      },
      "source": [
        "# We use the hog function from skimage.features directly. \n",
        "# So we don’t have to calculate the gradients, magnitude (total gradient) and orientation individually. \n",
        "# The hog function would internally calculate it and return the feature matrix.\n",
        "from skimage.feature import hog\n",
        "\n",
        "# Creating HOG feature Descriptor\n",
        "# First we pass the the gray scale image of horse\n",
        "# Set the parameter ‘visualize = True’, which return an image of the HOG\n",
        "fd, hog_horse = hog(horse_gray, visualize=True)\n",
        "\n",
        "# Plotting the original image\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Plotting the HOG image\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKi-rgXdgj9Q"
      },
      "source": [
        "From the above we can see the edges of the horse. Even it might be harder to identify a horse on the HOG image, compared to the original image, we can see that only the most important parts are preserved. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPOlPQArsqh7"
      },
      "source": [
        "# Joblib is an pacakge that can simply turn our Python code into parallel \n",
        "# computing mode and increases the computing speed\n",
        "# with the Parallel and delayed functions from Joblib, we can simply configure a parallel run of the below function\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Function which converts the train and test images to hog images\n",
        "def to_hog(images):\n",
        "    return hog(images)\n",
        "\n",
        "# Get to hog features from the train and test sets\n",
        "# n_jobs is the number of parallel jobs.\n",
        "# 'delayed(to_hog)(x) for x in hog_train' creates tuple of the function, x, and the parameters, one for each iteration. \n",
        "# Delayed creates these tuples, then Parallel will pass these to the interpreter.\n",
        "# Parallel(n_jobs=num_cores) does the heavy lifting of multiprocessing. \n",
        "# Parallel forks the Python interpreter into a number of processes equal to the number of jobs. \n",
        "# Each process will run one iteration, and return the result.\n",
        "\n",
        "hog_train = # YOUR CODE HERE\n",
        "hog_test = # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oEDIFYkoTwG"
      },
      "source": [
        "# Check for the shape of hog train and test features\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6hEN6aUoV0j"
      },
      "source": [
        "# Select classes #7 and #8 to extract the HOG features\n",
        "# Performing binary classification by selecting 2 classes from the data\n",
        "# The class 7 belongs to 'horse' and class 8 belongs to 'ship'\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7KbZxr5SmZf"
      },
      "source": [
        "# Check for the shape of HOG train and test features\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAWy89F2yLf5"
      },
      "source": [
        "### Train the classifier and get the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2FP7cIgl08I"
      },
      "source": [
        "# Call the functions to train the classifier and get the predictions\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX12j8eVweCm"
      },
      "source": [
        "# Print the accuracy on HOG features\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYojlg24wmpe"
      },
      "source": [
        "**observation:** From the above experiment, we observe that by using the HOG method we get a better representation of the features considering the important parts of the images. The accuracy for the HOG features is 92.6% using Linear Support Vector Classifier when compared to the original features directly passing to the classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvSYDW45txVn"
      },
      "source": [
        "### Model Evaluation on HOG features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTR3t-7227kG"
      },
      "source": [
        "\n",
        "#### Classification Report : \n",
        "\n",
        "A Classification report is used to measure the quality of predictions from a classification algorithm. More specifically, True Positives, False Positives, True negatives and False Negatives are used to predict the metrics of a classification report as shown below.\n",
        "\n",
        " * **true positive** The correct label of the given instance is positive, and the classifier also predicts it as a positive\n",
        " * **false positive** The correct label is negative, but the classifier incorrectly predicts it as positive\n",
        " * **true negative** The correct label is negative, and the classifier also predicts a negative\n",
        " * **false negative** The correct label is positive, but the classifier incorrectly predicts it as negative\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC0Q5QYl_yl-"
      },
      "source": [
        "* **Precision:** The precision is calculated as the ratio between the number of Positive samples correctly classified to the total number of samples classified as Positive (either correctly or incorrectly)\n",
        "\n",
        "    Precision = $\\mathbf{\\frac{True \\ Positive}{True \\ Positive + False \\ Positive}}$\n",
        "\n",
        "* **Recall:** Recall tells us how many true positives (points labelled as positive) were recalled or found by our model.\n",
        "\n",
        "   Recall = $\\mathbf{\\frac{True \\ Positive}{True \\ Positive + False \\ Negative}}$\n",
        "\n",
        "* **F1-score:** precision and recall can be combined into a single score that seeks to balance both concerns, called the F-score or the F-measure.\n",
        "  \n",
        "   F1-score = $ 2* \\mathbf{\\frac{Precision * Recall}{Precision + Recall}}$\n",
        "\n",
        "* **Accuracy:** it is the ratio of the number of correct predictions to the total number of input samples.\n",
        "\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/precision_and_recall.jpg\" alt=\"Drawing\" height=\"400\" width=\"360\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZhJlO0Tyr55"
      },
      "source": [
        "For more details on precision and Recall refer to the following [link](https://medium.com/@klintcho/explaining-precision-and-recall-c770eb9c69e9)\n",
        "\n",
        "For example of precision and Recall refer to the following [link](https://towardsdatascience.com/confusion-matrix-clearly-explained-fee63614dc7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPbsk_17u0vY"
      },
      "source": [
        "#### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnSKar6Xtu9K"
      },
      "source": [
        "# Print the classification report\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tFjlQvKrzSQ"
      },
      "source": [
        "#### Confusion Matrix:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV233WTv2EXp"
      },
      "source": [
        "\n",
        "* **Confusion matrix:**  is a table that is used to describe the performance of a classification model on a set of test data for which the true values are known. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4GBnOVuu8IC"
      },
      "source": [
        "mat = confusion_matrix(Y_test_hog, predictions_hog)\n",
        "\n",
        "plt.figure(figsize = (12,5))\n",
        "\n",
        "# Visualizing the confusion matrix as a heatmap\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmvdJ4aNmGjR"
      },
      "source": [
        "#@title Q.1. The values of x and their corresponding values of y, (x, y) are as follows: (0, 2), (1, 3), (2, 4), (3, 5), (4, 6). Find the least square regression line y = a x + b for the given values and estimate the value of y when x = 12. { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer1 = \"\" #@param [\"\",\"11.0\", \"13.0\",\"15.2\",\"14.0\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRwvwQ8_VbWR"
      },
      "source": [
        "#### Consider the following statement about SVM and answer Q2.\n",
        "\n",
        "A. SVM is a supervised machine learning algorithm, which cannot be used for classification but can be used for regression.\n",
        "\n",
        "B. SVM is an unsupervised machine learning algorithm, which can be used for classification but not for regression.\n",
        "\n",
        "C. SVM is a supervised machine learning algorithm, which can be used for classification as well as for regression.\n",
        "\n",
        "D. SVM is an unsupervised machine learning algorithm, which can be used for classification as well as for regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNsVIkpnDpLc",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. Which of the following is true for a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer2 = \"\" #@param [\"\",\"Only A\",\"Only C\",\"Both A and C\",\"Both B and D\",\"All of the above\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tpZisWbDZUI"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Z0w03yDZUJ"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkiuHlpxDZUJ"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1iTafi4DZUJ"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i9Lk8UjDZUK"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "i9ag4Eo0DZUK"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}